<html>
<head>
<title>Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly</title>
<link rel="SHORTCUT ICON" href="favicon.ico"/>
<link href='./css/paperstyle.css' rel='stylesheet' type='text/css'>
</head>

<body>

<div class="pageTitle">
    Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly
  <br>
  <br>

  <br>
  <span class = "Authors">
      <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://crtie.github.io/" target="_blank">Chenrui Tie</a><sup>1*</sup> &nbsp; &nbsp;
      <br>
       Yushi Du <sup>1*</sup> &nbsp; &nbsp;
       <a href="https://sxy7147.github.io/">Yan Zhao</a> <sup>1</sup> &nbsp; &nbsp;
      <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup> &nbsp; &nbsp;<br>
      <i>(*: indicates joint first authors)</i><br><br>
      <sup>1</sup> Peking University </a> &nbsp;
  </span>
  </div>
<br>
<br>
<br>
<div class = "material">
        <a target="_blank">[Paper]</a> &nbsp; &nbsp;
        <a target="_blank">[Code]</a> &nbsp; &nbsp;
        <a href="paper.bib" target="_blank">[BibTex]</a> &nbsp; &nbsp;
</div>

<div class = "abstractTitle">
  Abstract
</div>
<p class = "abstractText">
  Shape assembly aims to reassemble parts (or fragments) into a complete object, which is 
  a common task in our daily life.
  Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair),
  geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics.
  Instead of semantic information, this task focuses on geometric information of parts.
  As the both geometric and pose space of fractured parts are exceptionally large, shape pose disentanglement of part representations is beneficial to geometric shape assembly.
  In our paper, we propose to leverage SE(3) equivariance for such shape pose disentanglement.
  Moreover, while previous works in vision and robotics only consider SE(3) equivariance for the representations of single objects,
  we move a step forward and propose leveraging SE(3) equivariance for representations considering multi-part correlations,
  which further boosts the performance of the multi-part assembly.
  Experiments demonstrate the significance of SE(3) equivariance and our proposed method for geometric shape assembly.</p>

<br>
<div class="abstractTitle">
    Video Presentation
</div>
<br>
<center>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/pEtIAal-xgQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>



<br>
<br>
<br>

<div class="abstractTitle">
  Geometric Shape Assembly
</div>
  <img class="bannerImage" src="./images/teaser.png" ,="" width="600"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 1.
        <b>Geometric Shape Assembly</b> aims to assemble different fractured parts into a whole shape. We propose to leverage \textbf{SE(3) Equivariance} for learning Geometric Shape Assembly, which disentangles poses and shapes of fractured parts, and performs better than networks without SE(3)-equivariant representations.  </p></td></tr></tbody></table>

<div class="abstractTitle">
    Our Proposed Framework
</div>
  <img class="bannerImage" src="./images/framework.png" ,="" width="600"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 2.
        \caption{\textbf{Overview of our proposed framework.}
  Taking as input the point cloud of each part $i$,
  our framework first outputs the equivariant representation $F_i$ and invariant representation $G_i$,
  computes the correlation between part $i$ and each part $j$ using the matrix multiplication of $F_i$ and $G_j$, and thus gets each part's equivariant representation $H_i$ with part correlations.
  The rotation decoder and the translation decoder respectively take $H$ and decode the rotation and translation of each part.
  Additional constraints such as adversarial training and canonical point cloud reconstruction using $G$ further improves the performance of our method.  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/affordance.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 3.
        Learning placing and picking affordance with state
        ‘value’s for the future. Left to Right: The bottom black arrow
        indicates the manipulation (inference) order. Right to Left: Arrow flows show dependencies among placing affordance, picking
        affordance and ‘value’s. Given observation o, we select 3 picking
        points p1 p2 p3, and show how to supervise corresponding placing
        affordance Aplace
        o|p1 Aplace o|p2 Aplace o|p3 , and how to supervise Apick o on
        p1 p2 p3 using computed corresponding placing affordance.
  </p></td></tr></tbody></table>

  
<div class="abstractTitle">
    Qualitative Results
</div>
  <img class="bannerImage" src="./images/traj.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 4.
        Example action sequences for cable-ring, cable-ringnotarget, SpreadCloth and RopeConfiguration. White point denotes picking and black point denotes placing.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/vis_aff.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 5.
        Picking and placing affordance. Each row contains two (picking affordance, observation with ppick, placing affordance) tuples for a task. ppick is selected by picking affordance. Higher color temperature means higher affordance.

  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/vis_value.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 6.
        Visualization of ‘value’ shows that some states with closer distances to the target (e.g., larger area) may not have higher ‘value’, as these states are hard for future actions to fulfill the task.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/cmp_value.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 7.
        Placing affordance trained using ‘value’ supervision (red) and only using the greedy direct distance (blue).
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/real.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 8.
        Examples of real-world manipulation trajectories guided by picking and placing affordance.
  </p></td></tr></tbody></table>


<!-- <p></p>  -->



<br>
<br>


<div class = "abstractTitle">
  Citation
</div>

<!-- <div class="row" style="margin-top: 1em">
<div class="12u$ 1u$(xsmall)">   -->
<div class = "myBibTex">
  <pre style="background-color: #EBEBEB;">
@article{wu2023learning,
  title={Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation},
  author={Wu, Ruihai and Ning, Chuanruo and Dong, Hao},
  journal={arXiv preprint arXiv:2303.11057},
  year={2023}
}
  </pre>
</div>
<!-- </div> -->


<br>
<br>


<div class = "abstractTitle">
  Contact
</div>
<p class = "abstractText">
  If you have any questions, please feel free to contact <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a> at wuruihai_at_pku_edu_cn and <a href="https://tritiumr.github.io/" target="_blank">Chuanruo Ning</a> at chuanruo_at_pku_edu_cn.
</p>



<br>
<br>


</body></html>