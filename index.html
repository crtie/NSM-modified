<html>
<head>
<title>Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly</title>
<link rel="SHORTCUT ICON" href="favicon.ico"/>
<link href='./css/paperstyle.css' rel='stylesheet' type='text/css'>
</head>

<body>

<div class="pageTitle">
    Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly
  <br>
  <br>

  <br>
  <span class = "Authors">
      <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://crtie.github.io/" target="_blank">Chenrui Tie</a><sup>1*</sup> &nbsp; &nbsp;
      <br>
       Yushi Du <sup>1*</sup> &nbsp; &nbsp;
       <a href="https://sxy7147.github.io/">Yan Zhao</a> <sup>1</sup> &nbsp; &nbsp;
      <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup> &nbsp; &nbsp;<br>
      <i>(*: indicates joint first authors)</i><br><br>
      <sup>1</sup> Peking University </a> &nbsp;
  </span>
  </div>
<br>
<br>
<br>
<div class = "material">
        <a target="_blank">[Paper]</a> &nbsp; &nbsp;
        <a target="_blank">[Code]</a> &nbsp; &nbsp;
        <a href="paper.bib" target="_blank">[BibTex]</a> &nbsp; &nbsp;
</div>

<div class = "abstractTitle">
  Abstract
</div>
<p class = "abstractText">
  Shape assembly aims to reassemble parts (or fragments) into a complete object, which is 
  a common task in our daily life.
  Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair),
  geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics.
  Instead of semantic information, this task focuses on geometric information of parts.
  As the both geometric and pose space of fractured parts are exceptionally large, shape pose disentanglement of part representations is beneficial to geometric shape assembly.
  In our paper, we propose to leverage SE(3) equivariance for such shape pose disentanglement.
  Moreover, while previous works in vision and robotics only consider SE(3) equivariance for the representations of single objects,
  we move a step forward and propose leveraging SE(3) equivariance for representations considering multi-part correlations,
  which further boosts the performance of the multi-part assembly.
  Experiments demonstrate the significance of SE(3) equivariance and our proposed method for geometric shape assembly.</p>

<br>
<div class="abstractTitle">
    Video Presentation
</div>
<br>
<center>
    <iframe width="660" height="415" src="https://youtu.be/pEtIAal-xgQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>



<br>
<br>
<br>

<div class="abstractTitle">
    Deformable Object Manipulation
</div>
  <img class="bannerImage" src="./images/teaser.png" ,="" width="600"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 1.
        Deformable Object Manipulation has many difficulties. 1) It requires multiple steps to complete. 2) Most actions can
        hardly facilitate tasks, for the exceptionally complex states and
        dynamics. 3) Many local optimal states are temporarily closer to
        the target, but making following actions harder to coordinate for
        the whole task. We propose to learn Foresightful Dense Visual
        Affordance aware of future actions to avoid local optima for deformable object manipulation, with real-world implementations.
  </p></td></tr></tbody></table>

<div class="abstractTitle">
    Our Proposed Multi-stage Affordance Learning Framework
</div>
  <img class="bannerImage" src="./images/framework.png" ,="" width="600"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 2.
        Our proposed framework learns dense picking and placing affordance for deformable object manipulation (e.g., Unfolding Cloth). We collect multi-stage interaction data efficiently (Left) and learn proposed affordance stably in a multi-stage schema (Right) in the reversed task accomplishment order, from states close to the target to complex states.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/affordance.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 3.
        Learning placing and picking affordance with state
        ‘value’s for the future. Left to Right: The bottom black arrow
        indicates the manipulation (inference) order. Right to Left: Arrow flows show dependencies among placing affordance, picking
        affordance and ‘value’s. Given observation o, we select 3 picking
        points p1 p2 p3, and show how to supervise corresponding placing
        affordance Aplace
        o|p1 Aplace o|p2 Aplace o|p3 , and how to supervise Apick o on
        p1 p2 p3 using computed corresponding placing affordance.
  </p></td></tr></tbody></table>

  
<div class="abstractTitle">
    Qualitative Results
</div>
  <img class="bannerImage" src="./images/traj.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 4.
        Example action sequences for cable-ring, cable-ringnotarget, SpreadCloth and RopeConfiguration. White point denotes picking and black point denotes placing.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/vis_aff.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 5.
        Picking and placing affordance. Each row contains two (picking affordance, observation with ppick, placing affordance) tuples for a task. ppick is selected by picking affordance. Higher color temperature means higher affordance.

  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/vis_value.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 6.
        Visualization of ‘value’ shows that some states with closer distances to the target (e.g., larger area) may not have higher ‘value’, as these states are hard for future actions to fulfill the task.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/cmp_value.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 7.
        Placing affordance trained using ‘value’ supervision (red) and only using the greedy direct distance (blue).
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/real.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 8.
        Examples of real-world manipulation trajectories guided by picking and placing affordance.
  </p></td></tr></tbody></table>


<!-- <p></p>  -->



<br>
<br>


<div class = "abstractTitle">
  Citation
</div>

<!-- <div class="row" style="margin-top: 1em">
<div class="12u$ 1u$(xsmall)">   -->
<div class = "myBibTex">
  <pre style="background-color: #EBEBEB;">
@article{wu2023learning,
  title={Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation},
  author={Wu, Ruihai and Ning, Chuanruo and Dong, Hao},
  journal={arXiv preprint arXiv:2303.11057},
  year={2023}
}
  </pre>
</div>
<!-- </div> -->


<br>
<br>


<div class = "abstractTitle">
  Contact
</div>
<p class = "abstractText">
  If you have any questions, please feel free to contact <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a> at wuruihai_at_pku_edu_cn and <a href="https://tritiumr.github.io/" target="_blank">Chuanruo Ning</a> at chuanruo_at_pku_edu_cn.
</p>



<br>
<br>


</body></html>